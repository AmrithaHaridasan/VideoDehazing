{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0praceYhs2t"
      },
      "outputs": [],
      "source": [
        "#!pip install -q condacolab\n",
        "#import condacolab\n",
        "#condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!conda install pytorch=0.4.1 cuda90 -c pytorch"
      ],
      "metadata": {
        "id": "9ZBsYtK7kuJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/AOD-NET-demo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5uksnN-lYCL",
        "outputId": "313d779e-7bcb-45ac-842f-bd61b8dcad48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AOD-NET-demo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#! git clone https://github.com/MayankSingal/PyTorch-Image-Dehazing.git"
      ],
      "metadata": {
        "id": "V_Hd7qptncEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_nnCvMvoV6P",
        "outputId": "61dfcf85-d67f-4310-c886-e0ee8bdca387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id41XyiBpE6f",
        "outputId": "2c2ffbeb-47a7-436f-9cb9-0d680cfd6c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model on revide dataset\n",
        "\n",
        "!python3 \"/content/drive/MyDrive/AOD-NET-demo/PyTorch-Image-Dehazing/train.py\" --orig_images_path \"/content/drive/MyDrive/AOD-NET-demo/data/REVIDE/hazy_png/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPbfP9Xunv1o",
        "outputId": "422392e4-ed40-472b-a3ee-a1dcdab1ffa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training examples: 24524\n",
            "Total validation examples: 2780\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/content/drive/MyDrive/AOD-NET-demo/PyTorch-Image-Dehazing/train.py:54: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(dehaze_net.parameters(),config.grad_clip_norm)\n",
            "Loss at iteration 10 : 0.3634350597858429\n",
            "Loss at iteration 20 : 0.38783615827560425\n",
            "Loss at iteration 30 : 0.35253429412841797\n",
            "Loss at iteration 40 : 0.48244866728782654\n",
            "Loss at iteration 50 : 0.3511315882205963\n",
            "Loss at iteration 60 : 0.298403263092041\n",
            "Loss at iteration 70 : 0.3059597313404083\n",
            "Loss at iteration 80 : 0.35001301765441895\n",
            "Loss at iteration 90 : 0.25003722310066223\n",
            "Loss at iteration 100 : 0.25802454352378845\n",
            "Loss at iteration 110 : 0.21625319123268127\n",
            "Loss at iteration 120 : 0.2231089323759079\n",
            "Loss at iteration 130 : 0.1871512234210968\n",
            "Loss at iteration 140 : 0.14056894183158875\n",
            "Loss at iteration 150 : 0.18839311599731445\n",
            "Loss at iteration 160 : 0.1641673445701599\n",
            "Loss at iteration 170 : 0.20158495008945465\n",
            "Loss at iteration 180 : 0.17026804387569427\n",
            "Loss at iteration 190 : 0.15682369470596313\n",
            "Loss at iteration 200 : 0.17161114513874054\n",
            "Loss at iteration 210 : 0.17987719178199768\n",
            "Loss at iteration 220 : 0.18712809681892395\n",
            "Loss at iteration 230 : 0.16090980172157288\n",
            "Loss at iteration 240 : 0.16980619728565216\n",
            "Loss at iteration 250 : 0.16476894915103912\n",
            "Loss at iteration 260 : 0.16303373873233795\n",
            "Loss at iteration 270 : 0.1490335464477539\n",
            "Loss at iteration 280 : 0.17852893471717834\n",
            "Loss at iteration 290 : 0.14034871757030487\n",
            "Loss at iteration 300 : 0.16208408772945404\n",
            "Loss at iteration 310 : 0.17706669867038727\n",
            "Loss at iteration 320 : 0.16093717515468597\n",
            "Loss at iteration 330 : 0.15141561627388\n",
            "Loss at iteration 340 : 0.1682254523038864\n",
            "Loss at iteration 350 : 0.1771131455898285\n",
            "Loss at iteration 360 : 0.17079026997089386\n",
            "Loss at iteration 370 : 0.1732855588197708\n",
            "Loss at iteration 380 : 0.14889881014823914\n",
            "Loss at iteration 390 : 0.14914575219154358\n",
            "Loss at iteration 400 : 0.1858246624469757\n",
            "Loss at iteration 410 : 0.15910646319389343\n",
            "Loss at iteration 420 : 0.16625095903873444\n",
            "Loss at iteration 430 : 0.16544054448604584\n",
            "Loss at iteration 440 : 0.134090393781662\n",
            "Loss at iteration 450 : 0.16723579168319702\n",
            "Loss at iteration 460 : 0.1656716763973236\n",
            "Loss at iteration 470 : 0.2042621374130249\n",
            "Loss at iteration 480 : 0.15162745118141174\n",
            "Loss at iteration 490 : 0.18301500380039215\n",
            "Loss at iteration 500 : 0.1824149340391159\n",
            "Loss at iteration 510 : 0.14007164537906647\n",
            "Loss at iteration 520 : 0.1676052212715149\n",
            "Loss at iteration 530 : 0.1815967857837677\n",
            "Loss at iteration 540 : 0.1519184410572052\n",
            "Loss at iteration 550 : 0.16642256081104279\n",
            "Loss at iteration 560 : 0.16481560468673706\n",
            "Loss at iteration 570 : 0.13141296803951263\n",
            "Loss at iteration 580 : 0.1568453162908554\n",
            "Loss at iteration 590 : 0.1850302517414093\n",
            "Loss at iteration 600 : 0.16561613976955414\n",
            "Loss at iteration 610 : 0.14050744473934174\n",
            "Loss at iteration 620 : 0.15762116014957428\n",
            "Loss at iteration 630 : 0.14773790538311005\n",
            "Loss at iteration 640 : 0.1770929992198944\n",
            "Loss at iteration 650 : 0.1919914186000824\n",
            "Loss at iteration 660 : 0.15534737706184387\n",
            "Loss at iteration 670 : 0.1606818586587906\n",
            "Loss at iteration 680 : 0.16779494285583496\n",
            "Loss at iteration 690 : 0.15871365368366241\n",
            "Loss at iteration 700 : 0.14658541977405548\n",
            "Loss at iteration 710 : 0.16473592817783356\n",
            "Loss at iteration 720 : 0.15765917301177979\n",
            "Loss at iteration 730 : 0.17406301200389862\n",
            "Loss at iteration 740 : 0.1509772092103958\n",
            "Loss at iteration 750 : 0.1521085947751999\n",
            "Loss at iteration 760 : 0.14436447620391846\n",
            "Loss at iteration 770 : 0.15321391820907593\n",
            "Loss at iteration 780 : 0.14122836291790009\n",
            "Loss at iteration 790 : 0.20262207090854645\n",
            "Loss at iteration 800 : 0.15281671285629272\n",
            "Loss at iteration 810 : 0.13251090049743652\n",
            "Loss at iteration 820 : 0.16803677380084991\n",
            "Loss at iteration 830 : 0.16930992901325226\n",
            "Loss at iteration 840 : 0.171260267496109\n",
            "Loss at iteration 850 : 0.15802255272865295\n",
            "Loss at iteration 860 : 0.1576254665851593\n",
            "Loss at iteration 870 : 0.17250895500183105\n",
            "Loss at iteration 880 : 0.2098160833120346\n",
            "Loss at iteration 890 : 0.16089417040348053\n",
            "Loss at iteration 900 : 0.17486436665058136\n",
            "Loss at iteration 910 : 0.17303204536437988\n",
            "Loss at iteration 920 : 0.1526549607515335\n",
            "Loss at iteration 930 : 0.15494056046009064\n",
            "Loss at iteration 940 : 0.1840960681438446\n",
            "Loss at iteration 950 : 0.1554994136095047\n",
            "Loss at iteration 960 : 0.20243732631206512\n",
            "Loss at iteration 970 : 0.16330356895923615\n",
            "Loss at iteration 980 : 0.15483978390693665\n",
            "Loss at iteration 990 : 0.1782323569059372\n",
            "Loss at iteration 1000 : 0.13162867724895477\n",
            "Loss at iteration 1010 : 0.19322992861270905\n",
            "Loss at iteration 1020 : 0.17585940659046173\n",
            "Loss at iteration 1030 : 0.18608151376247406\n",
            "Loss at iteration 1040 : 0.16023759543895721\n",
            "Loss at iteration 1050 : 0.14758364856243134\n",
            "Loss at iteration 1060 : 0.16878999769687653\n",
            "Loss at iteration 1070 : 0.1534285843372345\n",
            "Loss at iteration 1080 : 0.19978874921798706\n",
            "Loss at iteration 1090 : 0.15813390910625458\n",
            "Loss at iteration 1100 : 0.17394551634788513\n",
            "Loss at iteration 1110 : 0.14035551249980927\n",
            "Loss at iteration 1120 : 0.1746925264596939\n",
            "Loss at iteration 1130 : 0.1502692699432373\n",
            "Loss at iteration 1140 : 0.13747943937778473\n",
            "Loss at iteration 1150 : 0.1560795158147812\n",
            "Loss at iteration 1160 : 0.16936737298965454\n",
            "Loss at iteration 1170 : 0.15958867967128754\n",
            "Loss at iteration 1180 : 0.14879567921161652\n",
            "Loss at iteration 1190 : 0.12663990259170532\n",
            "Loss at iteration 1200 : 0.17083200812339783\n",
            "Loss at iteration 1210 : 0.15945085883140564\n",
            "Loss at iteration 1220 : 0.16785970330238342\n",
            "Loss at iteration 1230 : 0.15160855650901794\n",
            "Loss at iteration 1240 : 0.1665450483560562\n",
            "Loss at iteration 1250 : 0.1940639168024063\n",
            "Loss at iteration 1260 : 0.1637880802154541\n",
            "Loss at iteration 1270 : 0.15257908403873444\n",
            "Loss at iteration 1280 : 0.15425565838813782\n",
            "Loss at iteration 1290 : 0.15218394994735718\n",
            "Loss at iteration 1300 : 0.17466968297958374\n",
            "Loss at iteration 1310 : 0.14823199808597565\n",
            "Loss at iteration 1320 : 0.1767372339963913\n",
            "Loss at iteration 1330 : 0.1801183819770813\n",
            "Loss at iteration 1340 : 0.14720143377780914\n",
            "Loss at iteration 1350 : 0.12616822123527527\n",
            "Loss at iteration 1360 : 0.17598459124565125\n",
            "Loss at iteration 1370 : 0.17738540470600128\n",
            "Loss at iteration 1380 : 0.1729143261909485\n",
            "Loss at iteration 1390 : 0.1566392183303833\n",
            "Loss at iteration 1400 : 0.1509116291999817\n",
            "Loss at iteration 1410 : 0.17327962815761566\n",
            "Loss at iteration 1420 : 0.15741972625255585\n",
            "Loss at iteration 1430 : 0.12959454953670502\n",
            "Loss at iteration 1440 : 0.13877898454666138\n",
            "Loss at iteration 1450 : 0.1656741499900818\n",
            "Loss at iteration 1460 : 0.1562637984752655\n",
            "Loss at iteration 1470 : 0.1516059935092926\n",
            "Loss at iteration 1480 : 0.18037830293178558\n",
            "Loss at iteration 1490 : 0.15306508541107178\n",
            "Loss at iteration 1500 : 0.15185607969760895\n",
            "Loss at iteration 1510 : 0.12961824238300323\n",
            "Loss at iteration 1520 : 0.14855144917964935\n",
            "Loss at iteration 1530 : 0.17034313082695007\n",
            "Loss at iteration 1540 : 0.16888181865215302\n",
            "Loss at iteration 1550 : 0.17919594049453735\n",
            "Loss at iteration 1560 : 0.15379001200199127\n",
            "Loss at iteration 1570 : 0.17723096907138824\n",
            "Loss at iteration 1580 : 0.14736804366111755\n",
            "Loss at iteration 1590 : 0.17942389845848083\n",
            "Loss at iteration 1600 : 0.18772418797016144\n",
            "Loss at iteration 1610 : 0.13651959598064423\n",
            "Loss at iteration 1620 : 0.15160737931728363\n",
            "Loss at iteration 1630 : 0.17034848034381866\n",
            "Loss at iteration 1640 : 0.19437935948371887\n",
            "Loss at iteration 1650 : 0.15024356544017792\n",
            "Loss at iteration 1660 : 0.16274741291999817\n",
            "Loss at iteration 1670 : 0.1605105996131897\n",
            "Loss at iteration 1680 : 0.17333538830280304\n",
            "Loss at iteration 1690 : 0.17362987995147705\n",
            "Loss at iteration 1700 : 0.14517392218112946\n",
            "Loss at iteration 1710 : 0.14834798872470856\n",
            "Loss at iteration 1720 : 0.1898442506790161\n",
            "Loss at iteration 1730 : 0.1581583470106125\n",
            "Loss at iteration 1740 : 0.1392175555229187\n",
            "Loss at iteration 1750 : 0.18747538328170776\n",
            "Loss at iteration 1760 : 0.15128423273563385\n",
            "Loss at iteration 1770 : 0.16259698569774628\n",
            "Loss at iteration 1780 : 0.1336834877729416\n",
            "Loss at iteration 1790 : 0.1712830662727356\n",
            "Loss at iteration 1800 : 0.1587328165769577\n",
            "Loss at iteration 1810 : 0.1540914624929428\n",
            "Loss at iteration 1820 : 0.1541825234889984\n",
            "Loss at iteration 1830 : 0.168113112449646\n",
            "Loss at iteration 1840 : 0.12495258450508118\n",
            "Loss at iteration 1850 : 0.13131290674209595\n",
            "Loss at iteration 1860 : 0.16428199410438538\n",
            "Loss at iteration 1870 : 0.16295640170574188\n",
            "Loss at iteration 1880 : 0.1506997048854828\n",
            "Loss at iteration 1890 : 0.19241629540920258\n",
            "Loss at iteration 1900 : 0.17313435673713684\n",
            "Loss at iteration 1910 : 0.17612715065479279\n",
            "Loss at iteration 1920 : 0.14651507139205933\n",
            "Loss at iteration 1930 : 0.16364973783493042\n",
            "Loss at iteration 1940 : 0.1633441150188446\n",
            "Loss at iteration 1950 : 0.18370378017425537\n",
            "Loss at iteration 1960 : 0.1524963080883026\n",
            "Loss at iteration 1970 : 0.1602211594581604\n",
            "Loss at iteration 1980 : 0.15657572448253632\n",
            "Loss at iteration 1990 : 0.1610589325428009\n",
            "Loss at iteration 2000 : 0.15770497918128967\n",
            "Loss at iteration 2010 : 0.16744782030582428\n",
            "Loss at iteration 2020 : 0.1836244910955429\n",
            "Loss at iteration 2030 : 0.20324720442295074\n",
            "Loss at iteration 2040 : 0.18766401708126068\n",
            "Loss at iteration 2050 : 0.13889631628990173\n",
            "Loss at iteration 2060 : 0.14914847910404205\n",
            "Loss at iteration 2070 : 0.16169996559619904\n",
            "Loss at iteration 2080 : 0.1608722060918808\n",
            "Loss at iteration 2090 : 0.18518519401550293\n",
            "Loss at iteration 2100 : 0.1482924371957779\n",
            "Loss at iteration 2110 : 0.18157310783863068\n",
            "Loss at iteration 2120 : 0.18022997677326202\n",
            "Loss at iteration 2130 : 0.16920989751815796\n",
            "Loss at iteration 2140 : 0.20119409263134003\n",
            "Loss at iteration 2150 : 0.16958267986774445\n",
            "Loss at iteration 2160 : 0.15350958704948425\n",
            "Loss at iteration 2170 : 0.1511106789112091\n",
            "Loss at iteration 2180 : 0.18980199098587036\n",
            "Loss at iteration 2190 : 0.1498592346906662\n",
            "Loss at iteration 2200 : 0.17683453857898712\n",
            "Loss at iteration 2210 : 0.1545337289571762\n",
            "Loss at iteration 2220 : 0.17166966199874878\n",
            "Loss at iteration 2230 : 0.15400171279907227\n",
            "Loss at iteration 2240 : 0.15324243903160095\n",
            "Loss at iteration 2250 : 0.1682480424642563\n",
            "Loss at iteration 2260 : 0.15502189099788666\n",
            "Loss at iteration 2270 : 0.18082039058208466\n",
            "Loss at iteration 2280 : 0.16032494604587555\n",
            "Loss at iteration 2290 : 0.1927645057439804\n",
            "Loss at iteration 2300 : 0.1547790914773941\n",
            "Loss at iteration 2310 : 0.13051660358905792\n",
            "Loss at iteration 2320 : 0.19026914238929749\n",
            "Loss at iteration 2330 : 0.1739910989999771\n",
            "Loss at iteration 2340 : 0.1721688210964203\n",
            "Loss at iteration 2350 : 0.20663677155971527\n",
            "Loss at iteration 2360 : 0.15663263201713562\n",
            "Loss at iteration 2370 : 0.17599666118621826\n",
            "Loss at iteration 2380 : 0.17704688012599945\n",
            "Loss at iteration 2390 : 0.1448337733745575\n",
            "Loss at iteration 2400 : 0.17192283272743225\n",
            "Loss at iteration 2410 : 0.17978554964065552\n",
            "Loss at iteration 2420 : 0.1631430834531784\n",
            "Loss at iteration 2430 : 0.17781545221805573\n",
            "Loss at iteration 2440 : 0.16018521785736084\n",
            "Loss at iteration 2450 : 0.1501917839050293\n",
            "Loss at iteration 2460 : 0.18689347803592682\n",
            "Loss at iteration 2470 : 0.1617388278245926\n",
            "Loss at iteration 2480 : 0.16777148842811584\n",
            "Loss at iteration 2490 : 0.16882765293121338\n",
            "Loss at iteration 2500 : 0.16292627155780792\n",
            "Loss at iteration 2510 : 0.18482239544391632\n",
            "Loss at iteration 2520 : 0.16154304146766663\n",
            "Loss at iteration 2530 : 0.16807396709918976\n",
            "Loss at iteration 2540 : 0.1956135332584381\n",
            "Loss at iteration 2550 : 0.1647084802389145\n",
            "Loss at iteration 2560 : 0.14039698243141174\n",
            "Loss at iteration 2570 : 0.1397516131401062\n",
            "Loss at iteration 2580 : 0.1754123717546463\n",
            "Loss at iteration 2590 : 0.17325416207313538\n",
            "Loss at iteration 2600 : 0.17977942526340485\n",
            "Loss at iteration 2610 : 0.18216603994369507\n",
            "Loss at iteration 2620 : 0.15831118822097778\n",
            "Loss at iteration 2630 : 0.20328372716903687\n",
            "Loss at iteration 2640 : 0.1477375030517578\n",
            "Loss at iteration 2650 : 0.18830005824565887\n",
            "Loss at iteration 2660 : 0.19099006056785583\n",
            "Loss at iteration 2670 : 0.15475918352603912\n",
            "Loss at iteration 2680 : 0.1838841736316681\n",
            "Loss at iteration 2690 : 0.16215406358242035\n",
            "Loss at iteration 2700 : 0.16993659734725952\n",
            "Loss at iteration 2710 : 0.15888182818889618\n",
            "Loss at iteration 2720 : 0.16095255315303802\n",
            "Loss at iteration 2730 : 0.17196819186210632\n",
            "Loss at iteration 2740 : 0.175309419631958\n",
            "Loss at iteration 2750 : 0.16558796167373657\n",
            "Loss at iteration 2760 : 0.17096324265003204\n",
            "Loss at iteration 2770 : 0.1464349925518036\n",
            "Loss at iteration 2780 : 0.1814429759979248\n",
            "Loss at iteration 2790 : 0.16748464107513428\n",
            "Loss at iteration 2800 : 0.18450382351875305\n",
            "Loss at iteration 2810 : 0.1463765799999237\n",
            "Loss at iteration 2820 : 0.14251431822776794\n",
            "Loss at iteration 2830 : 0.16221977770328522\n",
            "Loss at iteration 2840 : 0.17705677449703217\n",
            "Loss at iteration 2850 : 0.14594045281410217\n",
            "Loss at iteration 2860 : 0.16135969758033752\n",
            "Loss at iteration 2870 : 0.1651776283979416\n",
            "Loss at iteration 2880 : 0.1619240641593933\n",
            "Loss at iteration 2890 : 0.15427051484584808\n",
            "Loss at iteration 2900 : 0.1758890002965927\n",
            "Loss at iteration 2910 : 0.1767009198665619\n",
            "Loss at iteration 2920 : 0.1900997757911682\n",
            "Loss at iteration 2930 : 0.15034392476081848\n",
            "Loss at iteration 2940 : 0.16465437412261963\n",
            "Loss at iteration 2950 : 0.1680627465248108\n",
            "Loss at iteration 2960 : 0.16566330194473267\n",
            "Loss at iteration 2970 : 0.15352699160575867\n",
            "Loss at iteration 2980 : 0.1576053351163864\n",
            "Loss at iteration 2990 : 0.15584737062454224\n",
            "Loss at iteration 3000 : 0.1643557846546173\n",
            "Loss at iteration 3010 : 0.14912891387939453\n",
            "Loss at iteration 3020 : 0.14409753680229187\n",
            "Loss at iteration 3030 : 0.13541144132614136\n",
            "Loss at iteration 3040 : 0.19104722142219543\n",
            "Loss at iteration 3050 : 0.16621257364749908\n",
            "Loss at iteration 3060 : 0.1836891621351242\n",
            "Loss at iteration 10 : 0.16793301701545715\n",
            "Loss at iteration 20 : 0.20548082888126373\n",
            "Loss at iteration 30 : 0.14939503371715546\n",
            "Loss at iteration 40 : 0.19687093794345856\n",
            "Loss at iteration 50 : 0.1971205472946167\n",
            "Loss at iteration 60 : 0.1775481253862381\n",
            "Loss at iteration 70 : 0.17155468463897705\n",
            "Loss at iteration 80 : 0.14111223816871643\n",
            "Loss at iteration 90 : 0.14121481776237488\n",
            "Loss at iteration 100 : 0.173817440867424\n",
            "Loss at iteration 110 : 0.16573314368724823\n",
            "Loss at iteration 120 : 0.17018382251262665\n",
            "Loss at iteration 130 : 0.16742874681949615\n",
            "Loss at iteration 140 : 0.13449902832508087\n",
            "Loss at iteration 150 : 0.1637527495622635\n",
            "Loss at iteration 160 : 0.16244085133075714\n",
            "Loss at iteration 170 : 0.17327578365802765\n",
            "Loss at iteration 180 : 0.15724331140518188\n",
            "Loss at iteration 190 : 0.16125018894672394\n",
            "Loss at iteration 200 : 0.1548961102962494\n",
            "Loss at iteration 210 : 0.1901177018880844\n",
            "Loss at iteration 220 : 0.15382935106754303\n",
            "Loss at iteration 230 : 0.14677242934703827\n",
            "Loss at iteration 240 : 0.13762642443180084\n",
            "Loss at iteration 250 : 0.15289805829524994\n",
            "Loss at iteration 260 : 0.15427353978157043\n",
            "Loss at iteration 270 : 0.1932523250579834\n",
            "Loss at iteration 280 : 0.1518212854862213\n",
            "Loss at iteration 290 : 0.14384162425994873\n",
            "Loss at iteration 300 : 0.14384286105632782\n",
            "Loss at iteration 310 : 0.1868097335100174\n",
            "Loss at iteration 320 : 0.16380098462104797\n",
            "Loss at iteration 330 : 0.15586531162261963\n",
            "Loss at iteration 340 : 0.1403253972530365\n",
            "Loss at iteration 350 : 0.14605331420898438\n",
            "Loss at iteration 360 : 0.16158412396907806\n",
            "Loss at iteration 370 : 0.14303191006183624\n",
            "Loss at iteration 380 : 0.16434471309185028\n",
            "Loss at iteration 390 : 0.16716405749320984\n",
            "Loss at iteration 400 : 0.1615181565284729\n",
            "Loss at iteration 410 : 0.17364993691444397\n",
            "Loss at iteration 420 : 0.16138745844364166\n",
            "Loss at iteration 430 : 0.16328111290931702\n",
            "Loss at iteration 440 : 0.14151662588119507\n",
            "Loss at iteration 450 : 0.17880496382713318\n",
            "Loss at iteration 460 : 0.17800326645374298\n",
            "Loss at iteration 470 : 0.16448253393173218\n",
            "Loss at iteration 480 : 0.13725021481513977\n",
            "Loss at iteration 490 : 0.14038899540901184\n",
            "Loss at iteration 500 : 0.12981721758842468\n",
            "Loss at iteration 510 : 0.1739967167377472\n",
            "Loss at iteration 520 : 0.14682509005069733\n",
            "Loss at iteration 530 : 0.1296406090259552\n",
            "Loss at iteration 540 : 0.1592143326997757\n",
            "Loss at iteration 550 : 0.166754812002182\n",
            "Loss at iteration 560 : 0.15054047107696533\n",
            "Loss at iteration 570 : 0.14081501960754395\n",
            "Loss at iteration 580 : 0.13944998383522034\n",
            "Loss at iteration 590 : 0.15170630812644958\n",
            "Loss at iteration 600 : 0.16434597969055176\n",
            "Loss at iteration 610 : 0.1569451242685318\n",
            "Loss at iteration 620 : 0.15683655440807343\n",
            "Loss at iteration 630 : 0.14211173355579376\n",
            "Loss at iteration 640 : 0.19622230529785156\n",
            "Loss at iteration 650 : 0.17086029052734375\n",
            "Loss at iteration 660 : 0.15229231119155884\n",
            "Loss at iteration 670 : 0.15541011095046997\n",
            "Loss at iteration 680 : 0.16640539467334747\n",
            "Loss at iteration 690 : 0.16360318660736084\n",
            "Loss at iteration 700 : 0.16338877379894257\n",
            "Loss at iteration 710 : 0.1672220230102539\n",
            "Loss at iteration 720 : 0.1780434101819992\n",
            "Loss at iteration 730 : 0.1629422903060913\n",
            "Loss at iteration 740 : 0.1496196836233139\n",
            "Loss at iteration 750 : 0.18031640350818634\n",
            "Loss at iteration 760 : 0.18301165103912354\n",
            "Loss at iteration 770 : 0.20462045073509216\n",
            "Loss at iteration 780 : 0.16126607358455658\n",
            "Loss at iteration 790 : 0.15873274207115173\n",
            "Loss at iteration 800 : 0.16037370264530182\n",
            "Loss at iteration 810 : 0.15172132849693298\n",
            "Loss at iteration 820 : 0.16022153198719025\n",
            "Loss at iteration 830 : 0.13411349058151245\n",
            "Loss at iteration 840 : 0.17435890436172485\n",
            "Loss at iteration 850 : 0.15569904446601868\n",
            "Loss at iteration 860 : 0.18393296003341675\n",
            "Loss at iteration 870 : 0.13899490237236023\n",
            "Loss at iteration 880 : 0.16417962312698364\n",
            "Loss at iteration 890 : 0.17571821808815002\n",
            "Loss at iteration 900 : 0.1726548671722412\n",
            "Loss at iteration 910 : 0.15727831423282623\n",
            "Loss at iteration 920 : 0.1746342033147812\n",
            "Loss at iteration 930 : 0.1672692596912384\n",
            "Loss at iteration 940 : 0.16012848913669586\n",
            "Loss at iteration 950 : 0.13738514482975006\n",
            "Loss at iteration 960 : 0.15162009000778198\n",
            "Loss at iteration 970 : 0.15342000126838684\n",
            "Loss at iteration 980 : 0.17872656881809235\n",
            "Loss at iteration 990 : 0.1525118052959442\n",
            "Loss at iteration 1000 : 0.181686133146286\n",
            "Loss at iteration 1010 : 0.14593495428562164\n",
            "Loss at iteration 1020 : 0.17087292671203613\n",
            "Loss at iteration 1030 : 0.16651637852191925\n",
            "Loss at iteration 1040 : 0.07261435687541962\n",
            "Loss at iteration 1050 : 0.03093380108475685\n",
            "Loss at iteration 1060 : 0.01780609041452408\n",
            "Loss at iteration 1070 : 0.008111890405416489\n",
            "Loss at iteration 1080 : 0.020383009687066078\n",
            "Loss at iteration 1090 : 0.021526705473661423\n",
            "Loss at iteration 1100 : 0.021734734997153282\n",
            "Loss at iteration 1110 : 0.013405331410467625\n",
            "Loss at iteration 1120 : 0.009999051690101624\n",
            "Loss at iteration 1130 : 0.028192011639475822\n",
            "Loss at iteration 1140 : 0.019537119194865227\n",
            "Loss at iteration 1150 : 0.022501030936837196\n",
            "Loss at iteration 1160 : 0.009373867884278297\n",
            "Loss at iteration 1170 : 0.0258028507232666\n",
            "Loss at iteration 1180 : 0.016808344051241875\n",
            "Loss at iteration 1190 : 0.01679704152047634\n",
            "Loss at iteration 1200 : 0.011039409786462784\n",
            "Loss at iteration 1210 : 0.017278220504522324\n",
            "Loss at iteration 1220 : 0.007591680157929659\n",
            "Loss at iteration 1230 : 0.012960338965058327\n",
            "Loss at iteration 1240 : 0.010375567711889744\n",
            "Loss at iteration 1250 : 0.013384271413087845\n",
            "Loss at iteration 1260 : 0.027255618944764137\n",
            "Loss at iteration 1270 : 0.026609336957335472\n",
            "Loss at iteration 1280 : 0.014110096730291843\n",
            "Loss at iteration 1290 : 0.025843199342489243\n",
            "Loss at iteration 1300 : 0.013419374823570251\n",
            "Loss at iteration 1310 : 0.027091972529888153\n",
            "Loss at iteration 1320 : 0.015477025881409645\n",
            "Loss at iteration 1330 : 0.012829742394387722\n",
            "Loss at iteration 1340 : 0.03292025625705719\n",
            "Loss at iteration 1350 : 0.015996458008885384\n",
            "Loss at iteration 1360 : 0.01472566556185484\n",
            "Loss at iteration 1370 : 0.02886766754090786\n",
            "Loss at iteration 1380 : 0.01103375107049942\n",
            "Loss at iteration 1390 : 0.020639898255467415\n",
            "Loss at iteration 1400 : 0.011988368816673756\n",
            "Loss at iteration 1410 : 0.015381197445094585\n",
            "Loss at iteration 1420 : 0.012388587929308414\n",
            "Loss at iteration 1430 : 0.016990259289741516\n",
            "Loss at iteration 1440 : 0.009894250892102718\n",
            "Loss at iteration 1450 : 0.013586951419711113\n",
            "Loss at iteration 1460 : 0.012888154946267605\n",
            "Loss at iteration 1470 : 0.013548962771892548\n",
            "Loss at iteration 1480 : 0.013291005045175552\n",
            "Loss at iteration 1490 : 0.011812598444521427\n",
            "Loss at iteration 1500 : 0.01569700986146927\n",
            "Loss at iteration 1510 : 0.01685468479990959\n",
            "Loss at iteration 1520 : 0.014234579168260098\n",
            "Loss at iteration 1530 : 0.01206221990287304\n",
            "Loss at iteration 1540 : 0.02290257066488266\n",
            "Loss at iteration 1550 : 0.012132333591580391\n",
            "Loss at iteration 1560 : 0.018509281799197197\n",
            "Loss at iteration 1570 : 0.010795206762850285\n",
            "Loss at iteration 1580 : 0.014654611237347126\n",
            "Loss at iteration 1590 : 0.017909929156303406\n",
            "Loss at iteration 1600 : 0.016216514632105827\n",
            "Loss at iteration 1610 : 0.018148299306631088\n",
            "Loss at iteration 1620 : 0.014658989384770393\n",
            "Loss at iteration 1630 : 0.01853773184120655\n",
            "Loss at iteration 1640 : 0.027972672134637833\n",
            "Loss at iteration 1650 : 0.01291519496589899\n",
            "Loss at iteration 1660 : 0.01323998998850584\n",
            "Loss at iteration 1670 : 0.010983147658407688\n",
            "Loss at iteration 1680 : 0.012267635203897953\n",
            "Loss at iteration 1690 : 0.0139309698715806\n",
            "Loss at iteration 1700 : 0.017046207562088966\n",
            "Loss at iteration 1710 : 0.011638475582003593\n",
            "Loss at iteration 1720 : 0.01321376208215952\n",
            "Loss at iteration 1730 : 0.018894042819738388\n",
            "Loss at iteration 1740 : 0.01905755326151848\n",
            "Loss at iteration 1750 : 0.015382145531475544\n",
            "Loss at iteration 1760 : 0.013765850104391575\n",
            "Loss at iteration 1770 : 0.019933195784687996\n",
            "Loss at iteration 1780 : 0.014720690436661243\n",
            "Loss at iteration 1790 : 0.011091235093772411\n",
            "Loss at iteration 1800 : 0.01690180040895939\n",
            "Loss at iteration 1810 : 0.012762936763465405\n",
            "Loss at iteration 1820 : 0.021105261519551277\n",
            "Loss at iteration 1830 : 0.012712017633020878\n",
            "Loss at iteration 1840 : 0.015966249629855156\n",
            "Loss at iteration 1850 : 0.008353964425623417\n",
            "Loss at iteration 1860 : 0.013339625671505928\n",
            "Loss at iteration 1870 : 0.009577463380992413\n",
            "Loss at iteration 1880 : 0.008584976196289062\n",
            "Loss at iteration 1890 : 0.019668003544211388\n",
            "Loss at iteration 1900 : 0.023232748731970787\n",
            "Loss at iteration 1910 : 0.01815217174589634\n",
            "Loss at iteration 1920 : 0.01919218711555004\n",
            "Loss at iteration 1930 : 0.009478307329118252\n",
            "Loss at iteration 1940 : 0.009568867273628712\n",
            "Loss at iteration 1950 : 0.01587853953242302\n",
            "Loss at iteration 1960 : 0.012210709974169731\n",
            "Loss at iteration 1970 : 0.013785820454359055\n",
            "Loss at iteration 1980 : 0.031659048050642014\n",
            "Loss at iteration 1990 : 0.01408129371702671\n",
            "Loss at iteration 2000 : 0.01952279917895794\n",
            "Loss at iteration 2010 : 0.016736192628741264\n",
            "Loss at iteration 2020 : 0.01945384219288826\n",
            "Loss at iteration 2030 : 0.015522114001214504\n",
            "Loss at iteration 2040 : 0.011443216353654861\n",
            "Loss at iteration 2050 : 0.01991608552634716\n",
            "Loss at iteration 2060 : 0.008177923038601875\n",
            "Loss at iteration 2070 : 0.013541096821427345\n",
            "Loss at iteration 2080 : 0.013432241976261139\n",
            "Loss at iteration 2090 : 0.02269427478313446\n",
            "Loss at iteration 2100 : 0.011261692270636559\n",
            "Loss at iteration 2110 : 0.012038248591125011\n",
            "Loss at iteration 2120 : 0.010394453071057796\n",
            "Loss at iteration 2130 : 0.012972135096788406\n",
            "Loss at iteration 2140 : 0.017990803346037865\n",
            "Loss at iteration 2150 : 0.009013018570840359\n",
            "Loss at iteration 2160 : 0.026244617998600006\n",
            "Loss at iteration 2170 : 0.01211395114660263\n",
            "Loss at iteration 2180 : 0.016942571848630905\n",
            "Loss at iteration 2190 : 0.016286751255393028\n",
            "Loss at iteration 2200 : 0.016230285167694092\n",
            "Loss at iteration 2210 : 0.01643218845129013\n",
            "Loss at iteration 2220 : 0.011104206554591656\n",
            "Loss at iteration 2230 : 0.015561223961412907\n",
            "Loss at iteration 2240 : 0.013560301624238491\n",
            "Loss at iteration 2250 : 0.018217455595731735\n",
            "Loss at iteration 2260 : 0.013762995600700378\n",
            "Loss at iteration 2270 : 0.017280396074056625\n",
            "Loss at iteration 2280 : 0.017436260357499123\n",
            "Loss at iteration 2290 : 0.016035156324505806\n",
            "Loss at iteration 2300 : 0.024788839742541313\n",
            "Loss at iteration 2310 : 0.011471525765955448\n",
            "Loss at iteration 2320 : 0.019896041601896286\n",
            "Loss at iteration 2330 : 0.014517483301460743\n",
            "Loss at iteration 2340 : 0.009892513044178486\n",
            "Loss at iteration 2350 : 0.0116117550060153\n",
            "Loss at iteration 2360 : 0.008107749745249748\n",
            "Loss at iteration 2370 : 0.013896471820771694\n",
            "Loss at iteration 2380 : 0.011782779358327389\n",
            "Loss at iteration 2390 : 0.009397858753800392\n",
            "Loss at iteration 2400 : 0.02061554044485092\n",
            "Loss at iteration 2410 : 0.011305255815386772\n",
            "Loss at iteration 2420 : 0.006250614766031504\n",
            "Loss at iteration 2430 : 0.010988458059728146\n",
            "Loss at iteration 2440 : 0.015282913111150265\n",
            "Loss at iteration 2450 : 0.026377229019999504\n",
            "Loss at iteration 2460 : 0.009676823392510414\n",
            "Loss at iteration 2470 : 0.012807860039174557\n",
            "Loss at iteration 2480 : 0.015562718734145164\n",
            "Loss at iteration 2490 : 0.0216323584318161\n",
            "Loss at iteration 2500 : 0.010041503235697746\n",
            "Loss at iteration 2510 : 0.02474198304116726\n",
            "Loss at iteration 2520 : 0.012988236732780933\n",
            "Loss at iteration 2530 : 0.01840502768754959\n",
            "Loss at iteration 2540 : 0.013723030686378479\n",
            "Loss at iteration 2550 : 0.012631941586732864\n",
            "Loss at iteration 2560 : 0.008801450952887535\n",
            "Loss at iteration 2570 : 0.009939748793840408\n",
            "Loss at iteration 2580 : 0.030171021819114685\n",
            "Loss at iteration 2590 : 0.02315463311970234\n",
            "Loss at iteration 2600 : 0.02471204847097397\n",
            "Loss at iteration 2610 : 0.010104567743837833\n",
            "Loss at iteration 2620 : 0.01643563061952591\n",
            "Loss at iteration 2630 : 0.019915783777832985\n",
            "Loss at iteration 2640 : 0.014971906319260597\n",
            "Loss at iteration 2650 : 0.011725987307727337\n",
            "Loss at iteration 2660 : 0.016354378312826157\n",
            "Loss at iteration 2670 : 0.016884401440620422\n",
            "Loss at iteration 2680 : 0.012532792054116726\n",
            "Loss at iteration 2690 : 0.012874370440840721\n",
            "Loss at iteration 2700 : 0.014495586976408958\n",
            "Loss at iteration 2710 : 0.015468467026948929\n",
            "Loss at iteration 2720 : 0.008475210517644882\n",
            "Loss at iteration 2730 : 0.01681496389210224\n",
            "Loss at iteration 2740 : 0.020566927269101143\n",
            "Loss at iteration 2750 : 0.015261668711900711\n",
            "Loss at iteration 2760 : 0.014707868918776512\n",
            "Loss at iteration 2770 : 0.018602075055241585\n",
            "Loss at iteration 2780 : 0.014145703054964542\n",
            "Loss at iteration 2790 : 0.023261932656168938\n",
            "Loss at iteration 2800 : 0.022546030580997467\n",
            "Loss at iteration 2810 : 0.011201170273125172\n",
            "Loss at iteration 2820 : 0.02044602483510971\n",
            "Loss at iteration 2830 : 0.016588173806667328\n",
            "Loss at iteration 2840 : 0.009333201684057713\n",
            "Loss at iteration 2850 : 0.031593888998031616\n",
            "Loss at iteration 2860 : 0.013413121923804283\n",
            "Loss at iteration 2870 : 0.02338896133005619\n",
            "Loss at iteration 2880 : 0.01924208737909794\n",
            "Loss at iteration 2890 : 0.010255349799990654\n",
            "Loss at iteration 2900 : 0.022375235334038734\n",
            "Loss at iteration 2910 : 0.022422892972826958\n",
            "Loss at iteration 2920 : 0.00904892198741436\n",
            "Loss at iteration 2930 : 0.014549870043992996\n",
            "Loss at iteration 2940 : 0.01817259006202221\n",
            "Loss at iteration 2950 : 0.025936279445886612\n",
            "Loss at iteration 2960 : 0.012629522942006588\n",
            "Loss at iteration 2970 : 0.020983174443244934\n",
            "Loss at iteration 2980 : 0.019247861579060555\n",
            "Loss at iteration 2990 : 0.012627468444406986\n",
            "Loss at iteration 3000 : 0.016570601612329483\n",
            "Loss at iteration 3010 : 0.015049492008984089\n",
            "Loss at iteration 3020 : 0.010645431466400623\n",
            "Loss at iteration 3030 : 0.014897017739713192\n",
            "Loss at iteration 3040 : 0.01303015649318695\n",
            "Loss at iteration 3050 : 0.016286442056298256\n",
            "Loss at iteration 3060 : 0.016550926491618156\n",
            "Loss at iteration 10 : 0.016081852838397026\n",
            "Loss at iteration 20 : 0.01085042767226696\n",
            "Loss at iteration 30 : 0.010438473895192146\n",
            "Loss at iteration 40 : 0.013171653263270855\n",
            "Loss at iteration 50 : 0.017718316987156868\n",
            "Loss at iteration 60 : 0.030541805550456047\n",
            "Loss at iteration 70 : 0.01077333465218544\n",
            "Loss at iteration 80 : 0.014637460000813007\n",
            "Loss at iteration 90 : 0.012381971813738346\n",
            "Loss at iteration 100 : 0.01858501508831978\n",
            "Loss at iteration 110 : 0.016486341133713722\n",
            "Loss at iteration 120 : 0.009267019107937813\n",
            "Loss at iteration 130 : 0.03182131052017212\n",
            "Loss at iteration 140 : 0.01734086498618126\n",
            "Loss at iteration 150 : 0.020402925089001656\n",
            "Loss at iteration 160 : 0.01087663322687149\n",
            "Loss at iteration 170 : 0.021683625876903534\n",
            "Loss at iteration 180 : 0.014374216087162495\n",
            "Loss at iteration 190 : 0.010415308177471161\n",
            "Loss at iteration 200 : 0.008072689175605774\n",
            "Loss at iteration 210 : 0.01181472185999155\n",
            "Loss at iteration 220 : 0.021193034946918488\n",
            "Loss at iteration 230 : 0.01426848117262125\n",
            "Loss at iteration 240 : 0.010253572836518288\n",
            "Loss at iteration 250 : 0.010506905615329742\n",
            "Loss at iteration 260 : 0.016582323238253593\n",
            "Loss at iteration 270 : 0.01573234610259533\n",
            "Loss at iteration 280 : 0.009675834327936172\n",
            "Loss at iteration 290 : 0.021474774926900864\n",
            "Loss at iteration 300 : 0.014447941444814205\n",
            "Loss at iteration 310 : 0.014428024180233479\n",
            "Loss at iteration 320 : 0.011818468570709229\n",
            "Loss at iteration 330 : 0.01036720722913742\n",
            "Loss at iteration 340 : 0.01840979978442192\n",
            "Loss at iteration 350 : 0.012968583963811398\n",
            "Loss at iteration 360 : 0.012029396370053291\n",
            "Loss at iteration 370 : 0.013366514816880226\n",
            "Loss at iteration 380 : 0.011183544993400574\n",
            "Loss at iteration 390 : 0.01658371090888977\n",
            "Loss at iteration 400 : 0.015886304900050163\n",
            "Loss at iteration 410 : 0.016340062022209167\n",
            "Loss at iteration 420 : 0.019410964101552963\n",
            "Loss at iteration 430 : 0.012746047228574753\n",
            "Loss at iteration 440 : 0.01870722323656082\n",
            "Loss at iteration 450 : 0.016404276713728905\n",
            "Loss at iteration 460 : 0.020200178027153015\n",
            "Loss at iteration 470 : 0.018375087529420853\n",
            "Loss at iteration 480 : 0.018035490065813065\n",
            "Loss at iteration 490 : 0.010876724496483803\n",
            "Loss at iteration 500 : 0.021655481308698654\n",
            "Loss at iteration 510 : 0.021371696144342422\n",
            "Loss at iteration 520 : 0.026906590908765793\n",
            "Loss at iteration 530 : 0.012746786698698997\n",
            "Loss at iteration 540 : 0.012323867529630661\n",
            "Loss at iteration 550 : 0.011922654695808887\n",
            "Loss at iteration 560 : 0.008776905946433544\n",
            "Loss at iteration 570 : 0.018656985834240913\n",
            "Loss at iteration 580 : 0.010335990227758884\n",
            "Loss at iteration 590 : 0.01498620305210352\n",
            "Loss at iteration 600 : 0.0150424400344491\n",
            "Loss at iteration 610 : 0.017980355769395828\n",
            "Loss at iteration 620 : 0.013485139235854149\n",
            "Loss at iteration 630 : 0.020333118736743927\n",
            "Loss at iteration 640 : 0.010795338079333305\n",
            "Loss at iteration 650 : 0.024259543046355247\n",
            "Loss at iteration 660 : 0.017096811905503273\n",
            "Loss at iteration 670 : 0.009080344811081886\n",
            "Loss at iteration 680 : 0.008880557492375374\n",
            "Loss at iteration 690 : 0.015764379873871803\n",
            "Loss at iteration 700 : 0.010305376723408699\n",
            "Loss at iteration 710 : 0.013782434165477753\n",
            "Loss at iteration 720 : 0.011686377227306366\n",
            "Loss at iteration 730 : 0.008067678660154343\n",
            "Loss at iteration 740 : 0.017745349556207657\n",
            "Loss at iteration 750 : 0.01015138253569603\n",
            "Loss at iteration 760 : 0.010843337513506413\n",
            "Loss at iteration 770 : 0.025774795562028885\n",
            "Loss at iteration 780 : 0.01935630477964878\n",
            "Loss at iteration 790 : 0.01905471459031105\n",
            "Loss at iteration 800 : 0.008760194294154644\n",
            "Loss at iteration 810 : 0.015275044366717339\n",
            "Loss at iteration 820 : 0.01582263596355915\n",
            "Loss at iteration 830 : 0.00998490210622549\n",
            "Loss at iteration 840 : 0.023638345301151276\n",
            "Loss at iteration 850 : 0.011782577261328697\n",
            "Loss at iteration 860 : 0.009604369290173054\n",
            "Loss at iteration 870 : 0.014026308432221413\n",
            "Loss at iteration 880 : 0.014313033781945705\n",
            "Loss at iteration 890 : 0.013621201738715172\n",
            "Loss at iteration 900 : 0.016740689054131508\n",
            "Loss at iteration 910 : 0.009184096939861774\n",
            "Loss at iteration 920 : 0.01390937902033329\n",
            "Loss at iteration 930 : 0.01591583527624607\n",
            "Loss at iteration 940 : 0.01223925594240427\n",
            "Loss at iteration 950 : 0.012419641949236393\n",
            "Loss at iteration 960 : 0.010560845956206322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 \"/content/drive/MyDrive/AOD-NET-demo/PyTorch-Image-Dehazing/dehaze.py\""
      ],
      "metadata": {
        "id": "-WtFpbo8oUCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#THE RESULTS ARE STORED IN THE DEHAZE FOLDER AND HAVE BEEN DEMONSTRATED IN THE REPORT"
      ],
      "metadata": {
        "id": "hiiglGlrpIv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8tNe2qon_PGC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}